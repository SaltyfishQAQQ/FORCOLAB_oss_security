{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6bcfad-2e2b-4798-a0a7-d57366fce926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import github_fetching as fetcher\n",
    "import data_cleaning as cleaner\n",
    "import sentiment_analysis as analyzer\n",
    "import visualization as visualizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579fc7a-3e81-4a44-ab1e-a629dbfd55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_folder = \"inter_rater/hugging_face/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f1726-a275-4fdd-a57f-8bdde39f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher.fetch_issues_pr(repo_name = 'tukaani-project/xz', folder_location = os.path.join(hugging_face_folder, \"individual_issue_PR/\"), start_year = 2023, start_month = 1, start_day = 1, end_year = 2023, end_month = 12, end_day = 31)\n",
    "cleaner.clean_thread(folder_path = os.path.join(hugging_face_folder, \"individual_issue_PR/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67137227-4e77-4637-b889-03d82f2f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions = analyzer.sentence_sentiment_analysis(source_path = hugging_face_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d677f-9b67-412d-bff6-d2cca65cba4e",
   "metadata": {},
   "source": [
    "**IBM WATSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42c1ed-6879-43da-8098-088c95f81ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions, EmotionOptions\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8c645-f7ff-403e-a359-18540a1c2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uoft\n",
    "apikey_uoft=\"AmELNIND2iUq599YVQrEO6fhvGTThbnrWUk-5CkMNhwM\"\n",
    "url_uoft = \"https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/e4672fef-a075-4109-9250-f22368dce7cb\"\n",
    "\n",
    "# gmail\n",
    "apikey = 'dfaTQ4Cc3XVSEM41zmH4jOG0ZGxY8j_fp0Y2t01LoHtR'\n",
    "url = 'https://api.us-east.natural-language-understanding.watson.cloud.ibm.com/instances/51d76a3e-11e2-4540-b818-55bdedb8e7fa'\n",
    "\n",
    "# Set up the authenticator\n",
    "authenticator = IAMAuthenticator(apikey_uoft)\n",
    "\n",
    "# Set up the Natural Language Understanding instance\n",
    "nlu = NaturalLanguageUnderstandingV1(\n",
    "    version='2021-08-01',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "nlu.set_service_url(url_uoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174a4a1-5b50-48a9-a56d-e9e522fa3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_text_from_csv(directory_path, column_name='body'):\n",
    "    grouped_text = \"\"\n",
    "    column_name = 'body'\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    grouped_text += \"\".join(df[column_name].dropna()) + \"\\n\"\n",
    "\n",
    "    return grouped_text\n",
    "\n",
    "def analyze_sentiment_emotion(text, nlu_instance):\n",
    "    try:\n",
    "        # Check if the text is non-empty and has sufficient length\n",
    "        if len(text.strip()) < 10:  # Adjust the length threshold as needed\n",
    "            return 'N/A', 0.0, {}\n",
    "\n",
    "        response = nlu_instance.analyze(\n",
    "            text=text,\n",
    "            features=Features(\n",
    "                sentiment=SentimentOptions(),\n",
    "                emotion=EmotionOptions()\n",
    "            ),\n",
    "            language='en'\n",
    "        ).get_result()\n",
    "\n",
    "        sentiment = response['sentiment']['document']['label']\n",
    "        sentiment_score = response['sentiment']['document']['score']\n",
    "        emotions = response['emotion']['document']['emotion']\n",
    "\n",
    "        return sentiment, sentiment_score, emotions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        return 'Error', 0.0, {}\n",
    "\n",
    "def get_dominant_emotion(emotion_dict):\n",
    "    if isinstance(emotion_dict, dict):\n",
    "        # Find the emotion with the highest score\n",
    "        dominant_emotion = max(emotion_dict, key=emotion_dict.get)\n",
    "        return dominant_emotion\n",
    "    else:\n",
    "        return 'N/A'  # In case there's an issue with the emotion data\n",
    "\n",
    "def classify_pr_or_issue(filename):\n",
    "    if 'pr' in filename.lower():\n",
    "        return 'PR'\n",
    "    elif 'issue' in filename.lower():\n",
    "        return 'Issue'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def is_valid_author(author_name):\n",
    "    # Ensure the input is a string; if not, return False\n",
    "    if not isinstance(author_name, str):\n",
    "        return False\n",
    "    \n",
    "    # Check if the name contains only letters, spaces, hyphens, apostrophes, or digits\n",
    "    return bool(re.match(r'^[a-zA-Z0-9\\s\\'-]+$', author_name))\n",
    "\n",
    "def count_messages(column):\n",
    "    def safe_eval(x):\n",
    "        try:\n",
    "            return len(ast.literal_eval(x))\n",
    "        except (ValueError, SyntaxError):\n",
    "            return 0  # Return 0 if there's an error in parsing\n",
    "    return column.apply(safe_eval)\n",
    "\n",
    "def count_dataset(user_interactions):\n",
    "    num_pairs = len(user_interactions)\n",
    "    num_messages = 0\n",
    "    \n",
    "    for index, row in user_interactions.iterrows():\n",
    "        num_messages = num_messages + len(row['positive']) + len(row['negative']) + len(row['neutral'])\n",
    "        \n",
    "    print(\"Number of author pairs: \" + str(num_pairs))\n",
    "    print(\"Number of messages: \" + str(num_messages))\n",
    "\n",
    "def get_github_user_name(username, token = os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")):\n",
    "    url = f\"https://api.github.com/users/{username}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data.get('name') == None:\n",
    "            return username\n",
    "            \n",
    "        return data.get('name', username)  # Return username if the real name is not found\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data for username '{username}'. Status code: {response.status_code}\")\n",
    "        return username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346b64c-529f-48e1-8c94-e326655b352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tracker = defaultdict(lambda: {'positive': [], 'neutral': [], 'negative': []})\n",
    "\n",
    "def update_sentiment_tracker(source_author, target_in_message, sentiment, sentiment_score, original_message, filename):\n",
    "    if sentiment == 'positive':\n",
    "        sentiment_tracker[(source_author, target_in_message)]['positive'].append({\n",
    "            'message': original_message,\n",
    "            'score': sentiment_score,\n",
    "            'file_name' : filename\n",
    "        })\n",
    "    elif sentiment == 'neutral':\n",
    "        sentiment_tracker[(source_author, target_in_message)]['neutral'].append({\n",
    "            'message': original_message,\n",
    "            'score': sentiment_score,\n",
    "            'file_name' : filename\n",
    "        })\n",
    "    elif sentiment == 'negative':\n",
    "        sentiment_tracker[(source_author, target_in_message)]['negative'].append({\n",
    "            'message': original_message,\n",
    "            'score': sentiment_score,\n",
    "            'file_name' : filename\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe175c3-444f-469a-a47d-ae8aa17a36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_folder = \"inter_rater/ibm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46278a6-722f-46b7-96eb-f6fd89ad9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher.fetch_issues_pr(repo_name = 'tukaani-project/xz', folder_location = ibm_folder, start_year = 2023, start_month = 1, start_day = 1, end_year = 2023, end_month = 12, end_day = 31)\n",
    "cleaner.clean_thread(folder_path = os.path.join(ibm_folder, \"individual_issue_PR/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b32fa6-3726-4be8-8f73-c45083fa7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_pr_path = os.path.join(ibm_folder, \"individual_issue_PR/\")\n",
    "post_sentiment_results = []\n",
    "\n",
    "for filename in os.listdir(issue_pr_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(issue_pr_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            source_author = row['from']\n",
    "            target_in_message = row['to']\n",
    "            \n",
    "            # Analyze the sentiment of the message body\n",
    "            sentiment, sentiment_score, emotion = analyze_sentiment_emotion(row['body'], nlu)\n",
    "\n",
    "            # Update the sentiment count in the tracker with the original message\n",
    "            update_sentiment_tracker(source_author, target_in_message, sentiment, sentiment_score, row['body'],filename)\n",
    "\n",
    "        print(filename + \" is analyzed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896ba66-ca78-43d0-a962-e96c74645a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for (source_author, target_in_message), sentiments in sentiment_tracker.items():\n",
    "    result_list.append({\n",
    "        'from': source_author,\n",
    "        'to': target_in_message,\n",
    "        'positive': sentiments['positive'],\n",
    "        'neutral': sentiments['neutral'],\n",
    "        'negative': sentiments['negative']\n",
    "    })\n",
    "\n",
    "user_interactions = pd.DataFrame(result_list)\n",
    "\n",
    "directory = os.path.join(ibm_folder, \"deleted_user_interactions/\")\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "else:\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "\n",
    "# Step 1: Remove invalid 'to' values\n",
    "invalid_to = user_interactions[user_interactions['to'].isna()]\n",
    "user_interactions = user_interactions.dropna(subset=['to'])\n",
    "invalid_to.to_csv(os.path.join(directory, \"removed_invalid_to.csv\"), index=False)\n",
    "\n",
    "# Step 2: Remove self-interactions\n",
    "self_interactions = user_interactions[user_interactions['from'] == user_interactions['to']]\n",
    "user_interactions = user_interactions[user_interactions['from'] != user_interactions['to']]\n",
    "self_interactions.to_csv(os.path.join(directory, \"removed_self_interactions.csv\"), index=False)\n",
    "\n",
    "# Step 3: Filter out rows where authors are below the contribution threshold\n",
    "contribution_df = cleaner.calculate_author_contributions()\n",
    "contribution_df = contribution_df.sort_values(by='count', ascending=False)\n",
    "threshold = contribution_df['count'].describe()['50%']\n",
    "authors_to_eliminate = contribution_df[contribution_df['count'] < threshold]['from']\n",
    "\n",
    "below_threshold_interactions = user_interactions[\n",
    "    user_interactions['from'].isin(authors_to_eliminate) |\n",
    "    user_interactions['to'].isin(authors_to_eliminate)\n",
    "]\n",
    "user_interactions = user_interactions[\n",
    "    ~user_interactions['from'].isin(authors_to_eliminate) &\n",
    "    ~user_interactions['to'].isin(authors_to_eliminate)\n",
    "]\n",
    "\n",
    "below_threshold_interactions.to_csv(os.path.join(directory, \"removed_below_threshold_interactions.csv\"), index=False)\n",
    "\n",
    "print(\"Done cleaning for user interactions\")\n",
    "\n",
    "user_interactions = user_interactions.sort_values(by=['from'])\n",
    "user_interactions['from'] = user_interactions['from'].apply(get_github_user_name)\n",
    "user_interactions['to'] = user_interactions['to'].apply(get_github_user_name)\n",
    "print(\"Done GitHub username fetching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467be80-70a6-42c4-8f4e-7875f54427f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = analyzer.construct_individual_conversations(ibm_folder, user_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a134b206-098d-4573-b6f2-2031189ccd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_folder = \"inter_rater/ibm/individual_conversations\"\n",
    "hugging_face_folder = \"inter_rater/hugging_face/individual_conversations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e406da-3fab-4867-9e7e-c69a1af96dde",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get list of files in both folders\n",
    "ibm_files = sorted(os.listdir(ibm_folder))\n",
    "hf_files = sorted(os.listdir(hugging_face_folder))\n",
    "\n",
    "all_conversations = []\n",
    "\n",
    "# Iterate over the files in both folders\n",
    "for file1, file2 in zip(ibm_files, hf_files):\n",
    "    # Ensure the files correspond (by name)\n",
    "    if file1 != file2:\n",
    "        raise ValueError(f\"File mismatch: {file1} vs {file2}\")\n",
    "\n",
    "    # Load the files\n",
    "    df1 = pd.read_csv(os.path.join(ibm_folder, file1))\n",
    "    df2 = pd.read_csv(os.path.join(hugging_face_folder, file2))\n",
    "\n",
    "    mood1 = df1['mood']\n",
    "    mood2 = df2['mood']\n",
    "    \n",
    "\n",
    "    # Ensure moods are of the same length before comparing\n",
    "    if len(mood1) != len(mood2):\n",
    "        continue\n",
    "\n",
    "    # Track sentiments and mismatches\n",
    "    for i in range(len(mood1)):\n",
    "            all_conversations.append({\n",
    "                \"from\": df1['from'][i],\n",
    "                \"to\": df1['to'][i],\n",
    "                \"file_name\": df1['file_name'][i],\n",
    "                \"message\": df1['message'][i],\n",
    "                \"rater1_mood\": df1['mood'][i],\n",
    "                \"rater1_score\": df1['score'][i],\n",
    "                \"rater2_mood\": df2['mood'][i],\n",
    "                \"rater2_score\":df2['score'][i]\n",
    "            })\n",
    "\n",
    "all_conversations = pd.DataFrame(all_conversations);\n",
    "print(len(all_conversations))\n",
    "all_conversations.to_csv(\"inter_rater/hf_ibm_combined.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7046f656-d356-4cd9-b5af-483afcc7b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4afd48-044f-42a6-95ac-87683bf6f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_file = pd.read_csv(\"inter_rater/hf_ibm_chatgpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d15793-11f8-4c8e-b396-5c174e3575fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa: 0.5140614840795245\n",
      "\n",
      "IBM v.s. Hugging Face: 0.38161164359247435\n",
      "\n",
      "IBM v.s. Human: 0.4892921960072595\n",
      "\n",
      "Hugging Face v.s. Human: 0.7090209703437151\n"
     ]
    }
   ],
   "source": [
    "# rater1: IBM, rater2: Hugging Face, rater3: Myself\n",
    "ibm_hf_kappa_score = cohen_kappa_score(irr_file['rater1_mood'], irr_file['rater2_mood'])\n",
    "ibm_human_kappa_score = cohen_kappa_score(irr_file['rater1_mood'], irr_file['rater3_mood'])\n",
    "hf_human_kappa_score = cohen_kappa_score(irr_file['rater2_mood'], irr_file['rater3_mood'])\n",
    "\n",
    "# Create a matrix where each row represents a message, and columns represent counts of each sentiment\n",
    "ratings = []\n",
    "sentiment_options = ['positive', 'neutral', 'negative']\n",
    "\n",
    "for _, row in irr_file.iterrows():\n",
    "    # Count the occurrences of each sentiment for each message\n",
    "    sentiments = [row['rater1_mood'], row['rater2_mood'], row['rater3_mood']]\n",
    "    counts = [sentiments.count(sent) for sent in sentiment_options]\n",
    "    ratings.append(counts)\n",
    "\n",
    "# Convert ratings to a numpy array\n",
    "ratings_array = np.array(ratings)\n",
    "\n",
    "# Calculate Fleiss' Kappa\n",
    "fleiss_kappa_score = fleiss_kappa(ratings_array, method='fleiss')\n",
    "print(\"Fleiss' Kappa:\", fleiss_kappa_score)\n",
    "\n",
    "print(\"\\nIBM v.s. Hugging Face: \" + str(ibm_hf_kappa_score))\n",
    "print(\"\\nIBM v.s. Human: \" + str(ibm_human_kappa_score))\n",
    "print(\"\\nHugging Face v.s. Human: \" + str(hf_human_kappa_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a3564-2864-4651-bfc0-99a4aeea0289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
