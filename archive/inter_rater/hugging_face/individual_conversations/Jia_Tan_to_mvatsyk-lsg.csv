from,to,file_name,message,mood,score
Jia Tan,mvatsyk-lsg,pr_73.csv,"Hello!

Thanks for the PR. This is a great start to improving the fuzz testing. I will start with a few overall comments here and then add some more specific comments directly on the commits themselves.

First, we need to be sure that we are using the fuzz resources in the best way we can. Its easy to think of the OSS-Fuzz resources as unlimited, but each project can only be fuzzed so much. We should only include a fuzz target if it provides clear value and is testing an important part of liblzma that isn't being covered by a different fuzz target. Otherwise, less useful fuzz targets will take away compute time from the more useful ones. So, can you justify the reasoning behind each of the new fuzz targets? For instance, I am not sure that the raw encoder and decoder fuzz targets are useful since their important code paths are already covered by every other fuzz target. The raw coders don't have important header data, its just raw LZMA data. I am likely missing an important fuzz case, but in my mind I can think of three useful things to fuzz in our library:

- Metadata encoding/decoding (magic bytes, file headers, block headers, lzma2 chunk headers, etc.)
- Filter data encoding/decoding (LZMA1, BCJ, delta)
- Check functions (CRC32, CRC64, SHA256)

Next, the code itself has a lot of repeated boilerplate. Each of the fuzz targets has very little unique code. For instance, this could be reorganized into a shared header file that provides a function for encoding and a function for decoding. These functions can take the coder init function (lzma_alone_decoder(), lzma_auto_decoder(), etc.) as a function pointer arg and any needed flags or options.

We could also consider fuzzing the various BCJ filters (x86, PowerPC, ARM64, etc). These filters are designed to be applied to executable data, but will be run on non-executable data very often. So its possible that there are hidden data corruption bugs on an unexpected input sequence since they are mostly tested on executable data, making it a good candidate for fuzz testing. These filters cannot be used as raw coders at this time, so they will have to be combined in a filter chain with LZMA1/2. If we want to look for data corruption bugs, we should encode a chunk, then decode it and compare if the decoded version exactly matches the original data.

For your commit messages, we like to keep a consistent format. When we release, our Changelog is generated automatically from the contents of the commit messages. Also it helps us maintain our codebase better when the commit messages are descriptive and clear. For your commits, please have them start with the category of what they are changing. For these, I would prepend ""Tests:"" to the first line of each commit. The first line of each commit should be a brief description of the purpose of the commit. The following lines should explain what was changed and why. Make sure to wrap the lines of the commit message to at most 73 characters since different commit log viewers may or may not wrap long lines and it helps keep a consistent look in our Changelog.",positive,0.9773265372496098
Jia Tan,mvatsyk-lsg,pr_73.csv,"
Makes perfect sense. I noted the BCJ filter fuzzing as option to consider. We don't necessarily need to implement it or implement it right away. Just an idea of something we could also be fuzzing if we agree the value is there.


I think the simplest approach would be to use a common separate header file. Creating a dynamic template would take some extra build logic whereas an extra header file would only require updating the Makefile.",positive,0.6786818774417043
Jia Tan,mvatsyk-lsg,pr_73.csv,"
We like to keep our commits small and focused, so we will likely want more than one commit for this many changes. For now, don't worry about squashing your commits until the review is basically done. At the end we can figure out how many commits are appropriate for this and squash accordingly. So feel free to keep adding fix up commits as we go.

I'll start reviewing your new changes.",positive,0.266170269344002
Jia Tan,mvatsyk-lsg,pr_73.csv,"
Its safe to remove the `.lzma_raw` files and the `tests/files/README` changes.

Thanks for all the changes so far! I feel we are getting close to this being ready.",positive,0.9553973429137841
Jia Tan,mvatsyk-lsg,pr_73.csv,"
Thanks for looking into this!

After some thought, it seems like a better use of resources to omit `fuzz_decode_stream_crc.c` and `fuzz_decode_lzip.c`. If we think the CRC code should be fuzzed we can add a fuzz target to directly test the various check functions. I'm not sure this will be needed since the input data to the check functions doesn't have much impact on the code path taken. On the Lzip side, this feature isn't used much and the header is very simple. We have tests that cover this in the test framework already so it doesn't feel worth the resources to fuzz it when we already have two other fuzz targets that hit the interesting code paths (alone and stream decoder).

Instead, we should split `fuzz_encoder_stream` into two separate fuzz targets. The first could be called `fuzz_encode_stream` and the second `fuzz_encode_stream_light`. `fuzz_encode_stream` should use preset level 5 and `fuzz_encode_stream_light` should use preset level 1. 

After this, I think we are ready to squash the commits. As long as the commits are well organized it doesn't matter exactly how you choose to squash them. Here is one suggestion:

1. Move `fuzz.c` to `fuzz_decode_stream.c`
2. Separate logic from `fuzz_decode_stream.c` into `fuzz_common.h`
3. Makefile changes
4. Add `fuzz_decode_alone` fuzz target
5. Add `fuzz_encode_stream` fuzz target
6. Add `fuzz_encode_stream_light` fuzz target

Commits 5 and 6 could be combined, up to you.",neutral,0.14145348593592644
Jia Tan,mvatsyk-lsg,pr_73.csv,"
I believe what was meant was that we have built up a very large corpus over the years on the `fuzz.c` fuzz target. Since that is renamed to `fuzz_decode_stream.c` in this PR, we would lose that large corpus if we do not take the proper steps to prevent that. We can either not rename this fuzz target or download a copy and restore it. I prefer the latter, and I have already downloaded a recent version of the corpus so it can be restored later.


I don't see us incorporating CIFuzz since features get integrated into OSS-Fuzz soon after they are committed anyways. The real question is how OSS-Fuzz divides up time between fuzz targets. I have not seen any description of this on the OSS-Fuzz online documentation so we would likely have to look into their internals to truly answer that question.


The point here is that we don't want to over-emphasize the importance of code coverage. Fuzzing is computationally expensive so increasing the code coverage should only be done if we are increasing **meaningful** code coverage. I would much rather fuzz 1 complicated function that 10 simple ones.

So the goal shouldn't be to hit a certain percentage of code coverage. The goal should be to fuzz 100% of critical complicated code. And we don't expect you to know what all the critical complicated code in our project is, thats where we need to work together.

We do appreciate your efforts so far. I know it doesn't feel great to remove things, but in this case less is more.

The LZMA encoder certainly counts as critical complex code and that fuzz target adds a lot of value :)


Each fuzz target needs to provide justifiable value outside of just extra code coverage. With the above points in mind, it feels safe to only consider the following fuzz targets:

- fuzz_decode_alone. This test focuses on LZMA1 header/data fuzzing and EOPM handling.

- fuzz_decode_lzip. This test focuses on LZIP header fuzzing. The LZIP header parsing is fairly simple and we have tests for it in the test framework, but maybe its still worth fuzzing.

- fuzz_decode_stream_crc. This test focuses on portions not covered by fuzz_decode_stream. So mainly when the code leading to the check functions and the check functions themselves. The check functions are complicated and may deserve fuzzing, but the code leading to them may not really need fuzzing. So a more focused fuzz target that just calls the CRC and SHA256 functions directly could be more efficient since it avoids the LZMA decoding (which is covered by the other fuzzers).

- fuzz_decode_stream. This test focuses on fuzzing .xz headers, block headers, index, etc. Additionally it fuzzes LZMA2 and LZMA1 decoding.

- fuzz_encode_stream. This test focuses on encoding a .xz file including all of the things in fuzz_decode_stream, except on the encoder side.

There are a few areas where fuzzing could be expanded if we agree these are critical complex code paths:

- BCJ Filters (already partially covered by fuzz_decode_stream)

- Delta Filter (already partially covered by fuzz_decode_stream)

- Different encoder settings (different match finders, dictionary sizes, LZMA properties, etc.). This could be accomplished by having an additional fuzz target for the encoder using preset 1. Instead of the default preset in `fuzz_encode_stream.c` we could use preset 5 since it should be a little faster (smaller `nice_len`) but have almost all the same settings. 

The fuzzer machines do not have multiple cores, so unfortunately it doesn't make sense to fuzz the multithreaded stream encoder/decoder code. Otherwise that would be another candidate for critical complex code.",negative,-0.24310262687504292
